xlim(5,18)
set.seed(110)
data_1 = tibble(measure=rnorm(40, 10, 1))
data_1$site_name <- 'Site 1'
data_2 = tibble(measure=rnorm(40, 10, 1))
data_2$site_name <- 'Site 2'
complete_data = rbind(data_1, data_2)
ggplot(complete_data, aes(x=measure, fill = site_name)) +
geom_density(alpha = 0.4, bw =0.60) +
xlim(5,18)
ratios =c(0.60, 0.61, 0.19, 0.19, 0.21,0.20)
coral_proportions = tibble(coral_sp, ratios)
coral_sp = c("P. lobata","P. lobata", "M. capitata", "M. capitata", "P. mendrina","P. mendrina")
ratios =c(0.60, 0.61, 0.19, 0.19, 0.21,0.20)
coral_proportions = tibble(coral_sp, ratios)
coral_multinom = function(){
true_proportions = c(0.60,0.19,0.21)
sample_proportions = rmultinom(1, 200, prob=c(0.61,0.19,0.20))/200
sample_tvd = sum(abs(true_proportions- sample_proportions))/2
sample_tvd
}
sampleOneIteration()
coral_multinom()
coral_multinom = function(){
true_proportions = c(0.60,0.19,0.21)
sample_proportions = rmultinom(1, 200, prob=c(0.61,0.19,0.20))/200
sample_tvd = sum(abs(true_proportions- sample_proportions))/2
sample_tvd
}
coral_multinom()
coral_multinom()
tvds = replicate(1000, coral_multinom())
p_value
p_value = sum(tvds > observed_stat)  / length(tvds)
p_value
p_value = sum(tvds > observed_stat)  / length(tvds)
observed_stat = sum(tapply(coral_proportions$ratios, coral_proportions$coral_sp, subtract_abs))/2
subtract_abs = function(x){
abs(x[1] - x[2])
}
observed_stat = sum(tapply(coral_proportions$ratios, coral_proportions$coral_sp, subtract_abs))/2
tvds = replicate(1000, coral_multinom())
p_value = sum(tvds > observed_stat)  / length(tvds)
p_value
sum(tapply(coral_proportions$ratios, coral_proportions$coral_sp, subtract_abs))/2
observed_stat = sum(tapply(coral_proportions$ratios, coral_proportions$coral_sp, subtract_abs))/2
tvds = replicate(1000, coral_multinom())
p_value = sum(tvds > observed_stat)/length(tvds)
p_value
ggplot()+
geom_histogram(aes(tvds, ..density..), bins = 15) +
geom_point(aes(observed_stat, 0), size=5, color="red")
complete_data = rbind(data_1, data_2)
mean(new_data_1) - mean(new_data_2)
Bootstrap_concat = function(data_1, data_2){
concat_data = c(data_1, data_2)
len_concat_data = length(concat_data)
len_data_1 = length(data_1)
shuffled_data = sample(concat_data)
new_data_1 = shuffled_data[1:len_data_1]
new_data_2 = shuffled_data[(len_data_1+1):len_concat_data]
mean(new_data_1) - mean(new_data_2)
}
processOneIter(data_1, data_2)
Bootstrap_concat(data_1, data_2)
observed_value
observed_value = mean(data_1) - mean(data_2)
Bootstrap_concat = function(data_1$measure, data_2$measure){
concat_data = c(data_1, data_2)
len_concat_data = length(concat_data)
len_data_1 = length(data_1)
shuffled_data = sample(concat_data)
new_data_1 = shuffled_data[1:len_data_1]
new_data_2 = shuffled_data[(len_data_1+1):len_concat_data]
mean(new_data_1) - mean(new_data_2)
}
Bootstrap_concat = function(data_1$measure, data_2$measure){
concat_data = c(data_1, data_2)
len_concat_data = length(concat_data)
len_data_1 = length(data_1)
shuffled_data = sample(concat_data)
new_data_1 = shuffled_data[1:len_data_1]
new_data_2 = shuffled_data[(len_data_1+1):len_concat_data]
mean(new_data_1) - mean(new_data_2)
}
Bootstrap_concat = function(data_1, data_2){
concat_data = c(data_1, data_2)
len_concat_data = length(concat_data)
len_data_1 = length(data_1)
shuffled_data = sample(concat_data)
new_data_1 = shuffled_data[1:len_data_1]
new_data_2 = shuffled_data[(len_data_1+1):len_concat_data]
mean(new_data_1) - mean(new_data_2)
}
Bootstrap_concat(data_1, data_2)
Bootstrap_concat = function(data_1, data_2){
concat_data = c(data_1, data_2)
len_concat_data = length(concat_data)
len_data_1 = length(data_1)
shuffled_data = sample(concat_data)
new_data_1 = shuffled_data[1:len_data_1]
new_data_2 = shuffled_data[(len_data_1+1):len_concat_data]
mean(new_data_1) - mean(new_data_2)
}
Bootstrap_concat(data_1, data_2)
Bootstrap_concat(data_1$measure, data_2$measure)
bootstrap_concat = function(data_1, data_2){
concat_data = c(data_1, data_2)
len_concat_data = length(concat_data)
len_data_1 = length(data_1)
shuffled_data = sample(concat_data)
new_data_1 = shuffled_data[1:len_data_1]
new_data_2 = shuffled_data[(len_data_1+1):len_concat_data]
mean(new_data_1) - mean(new_data_2)
}
bootstrap_concat(data_1$measure, data_2$measure)
bootstrap_concat(data_1$measure, data_2$measure)
observed_value = mean(data_1) - mean(data_2)
observed_value = mean(data_1$measure) - mean(data_2$measure)
observed_value
mean_under_null = replicate(1000, bootstrap_concat(data_1$measure, data_2$measure))
ggplot()+
geom_histogram(aes(mean_under_null, y=..density..), bins =30, alpha=0.2, color="black") +
geom_density(aes(mean_under_null), bw=0.2, size=0.5) +
xlim(-5, 5) +
geom_point(aes(observed_value, 0), color="red", size=10)
ggplot()+
geom_histogram(aes(mean_under_null, y=..density..), bins =30, alpha=0.2, color="black") +
geom_density(aes(mean_under_null), bw=0.2, size=0.5) +
xlim(-5, 5) +
geom_point(aes(observed_value, 0), color="red", size=5)
ggplot()+
geom_histogram(aes(mean_under_null, y=..density..), bins =30, alpha=0.2, color="black") +
geom_density(aes(mean_under_null), bw=0.5, size=0.5) +
xlim(-5, 5) +
geom_point(aes(observed_value, 0), color="red", size=5)
ggplot()+
geom_histogram(aes(mean_under_null, y=..density..), bins =30, alpha=0.2, color="black") +
geom_density(aes(mean_under_null), bw=0.06, size=0.5) +
xlim(-5, 5) +
geom_point(aes(observed_value, 0), color="red", size=5)
ggplot()+
geom_histogram(aes(mean_under_null, y=..density..), bins =30, alpha=0.2, color="black") +
geom_density(aes(mean_under_null), bw=0.05, size=0.5) +
xlim(-5, 5) +
geom_point(aes(observed_value, 0), color="red", size=5)
sum(mean_under_null <= observed_value) / length(mean_under_null)
source("C:/Users/Sam/Desktop/Data science in R/Weekly practicals_Shedd/Week_7_rstudio.R")
library(factoextra)
library(geometry)
install.packages("geometry")
install.packages("factoextra")
x = rnorm(30, 5, 2)
y = rnorm(30, 0, 2)
y = rnorm(45, 1, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(30, 0, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(45, 5, 2)
#Q3. Y and Z should have a mild correlation ( > 0.9)
z = 1.1 * y + rnorm(45, 1, 0.5)
#Q4. Generate a variable outcome as a linear combination of W, X, Y, and Z
random_data = data.frame(x, y, z, w)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(45, 5, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(45, 1, 2)
x = rnorm(45, 5, 2)
y = rnorm(45, 1, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(45, 1, 2)
#Q3. Y and Z should have a mild correlation ( > 0.9)
z = 1.1 * y + rnorm(45, 1, 0.5)
#Q4. Generate a variable outcome as a linear combination of W, X, Y, and Z
df_xwyz = data.frame(x, y, z, w)
#Q4. Generate a variable outcome as a linear combination of W, X, Y, and Z
df_wxyz = data.frame(x, y, z, w)
head(df_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x ~ y, df_wxyz)
summary(lm_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(w~x~y~z, df_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x~., data = df_wxyz[1:4,])
summary(lm_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x~y, data = df_wxyz)
summary(lm_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x~w, data = df_wxyz)
summary(lm_wxyz)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(30, 0, 2)
#Q3. Y and Z should have a mild correlation ( > 0.9)
z = 1.1 * y + rnorm(30, 0, 0.2)
x = rnorm(45, 5, 2)
y = rnorm(45, 1, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(30, 0, 2)
#Q1. X and Y should not be correlated
#They are independent
set.seed(44)
x = rnorm(30, 5, 2)
y = rnorm(30, 1, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.4 * x + rnorm(30, 0, 2)
#Q3. Y and Z should have a mild correlation ( > 0.9)
z = 1.1 * y + rnorm(30, 0, 0.2)
#Q4. Generate a variable outcome as a linear combination of W, X, Y, and Z
df_wxyz = data.frame(x, y, z, w)
head(df_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x~w, data = df_wxyz)
summary(lm_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(y~z, data = df_wxyz)
summary(lm_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wx = lm(x~w, data = df_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_yz = lm(y~z, df_wxyz)
summary(lm_yz, lm_wx)
summary(lm_yz)
lm_xw = lm(x~w, df_wxyz)
lm_xy = lm(y~x, df_wxyz)
summary(lm_xy)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
library(tidyverse)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
install.packages("geometry")
install.packages("factoextra")
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
library(moderndive)
install.packages("moderndive")
library(moderndive)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(x, w)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(y, z)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(x, w)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
library(geometry)
library(factoextra)
#Q6. Use PCA to reduce the dimensionality of your dataset
#Can you explain why you don't need to include the outcome?
ev = eigen(df_wxyz)
ev$vectors
#Q6. Use PCA to reduce the dimensionality of your dataset
#Can you explain why you don't need to include the outcome?
ev = eigen(df_wxyz)
#Q6. Use PCA to reduce the dimensionality of your dataset
#Can you explain why you don't need to include the outcome?
pca_wxyz = prcomp(df_wxyz, scale=TRUE)
pca_wxyz$rotation
pca_wxyz$sdev^2 / sum(pca_wxyz$sdev^2)
#Q7. Use the bi-plot to visualize the contributions of your initial variables
fviz_pca_biplot(pca_wxyz)
#Q1. X and Y should not be correlated
#They are independent
set.seed(44)
x = rnorm(30, 5, 2)
y = rnorm(30, 1, 2)
#Q2. W and X should have a mild correlation ( < 0.5)
w = 0.45 * x + rnorm(30, 0, 2)
#Q3. Y and Z should have a mild correlation ( > 0.9)
z = 1.05 * y + rnorm(30, 0, 0.2)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x~y, df_wxyz)
#Q4. Generate a variable outcome as a linear combination of W, X, Y, and Z
df_wxyz = data.frame(x, y, z, w)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_wxyz = lm(x~y, df_wxyz)
summary(lm_wxyz)
#Q5. Model your outcome using W, X, Y, and Z.
#Do your results match your model params?
lm_xy = lm(x~y, df_wxyz)
lm_yz = lm(y~z, df_wxyz)
lm_xw = lm(x~w, df_wxyz)
summary(lm_yz)
summary(lm_xw)
#Q6. Use PCA to reduce the dimensionality of your dataset
#Can you explain why you don't need to include the outcome?
pca_wxyz = prcomp(df_wxyz, scale=TRUE)
summary(pca_wxyz)
res_pca$sdev^2 / sum(res_pca$sdev^2)
pca_wxyz$sdev^2 / sum(pca_wxyz$sdev^2)
#Q7. Use the bi-plot to visualize the contributions of your initial variables
fviz_pca_biplot(pca_wxyz)
ggplot(df_wxyz, aes(x, y)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(x, w)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
ggplot(df_wxyz, aes(y, z)) +
geom_point() +
geom_parallel_slopes(se=FALSE)
#Q7. Use the bi-plot to visualize the contributions of your initial variables
fviz_pca_biplot(pca_wxyz)
str(pca_wxyz)
View(df_wxyz)
survey_data$richness = rowSums(survey_data[,2:76]>0)
setwd("C:\\Users\\Sam\\Desktop\\Data science in R\\Weekly practicals_Shedd")
getwd()
survey_data <- read.csv("RIKZ.txt", sep="\t", header=TRUE)
survey_data$richness = rowSums(survey_data[,2:76]>0)
survey_data_richness <- survey_data[,77:90]
View(survey_data_richness)
summary_reg_richness
reg_richness = regsubsets(richness ~ ., survey_data_richness, nvmax = 15)
library(pls)
reg_richness = regsubsets(richness ~ ., survey_data_richness, nvmax = 15)
summary_reg_richness = summary(reg_richness)
summary_reg_richness
library (leaps)
library(pls)
library(tidyverse)
# indicates that the best 2-variable model contains NAP and chalk
summary_reg_richness$rsq
reg_richness = regsubsets(richness ~ ., survey_data_richness, nvmax = 15)
summary_reg_richness = summary(reg_richness)
summary_reg_richness
# indicates that the best 2-variable model contains NAP and chalk
summary_reg_richness$rsq
# plot RSS for all the models
plot(summary_reg_richness$rss , xlab = " Number of Variables ",
ylab = " RSS ", type = "l")
# plot adjust R^2 for all the models
plot(summary_reg_richness$adjr2 , xlab = " Number of Variables ",
ylab = " Adjusted RSq ", type = "l")
# identify location of the max point of a vector
which.max(summary_reg_richness$adjr2)
which.max(summary_reg_richness$adjr2)
# 4
points(4, summary_reg_richness$adjr2[4], col = " red ", cex = 2,
pch = 20)
species_cols=2:76
species_cols
counts = apply(survey_data[, species_cols] > 0, 1, sum)
survey_data["richness"] = counts
View(survey_data)
# create copy of data set without species data
survey_data_richness = survey_data[,-species_cols]
View(survey_data_richness)
survey_data$richness = rowSums(survey_data[,2:76]>0)
survey_data_richness <- survey_data[,77:90]
reg_richness = regsubsets(richness ~ ., survey_data_richness, nvmax = 15)
summary_reg_richness = summary(reg_richness)
# indicates that the best 2-variable model contains NAP and chalk
summary_reg_richness$rsq
# plot RSS for all the models
plot(summary_reg_richness$rss , xlab = " Number of Variables ",
ylab = " RSS ", type = "l")
# plot adjust R^2 for all the models
plot(summary_reg_richness$adjr2 , xlab = " Number of Variables ",
ylab = " Adjusted RSq ", type = "l")
which.max(summary_reg_richness$adjr2)
# 4
points(4, summary_reg_richness$adjr2[4], col = " red ", cex = 2,
pch = 20)
# plot Cp and BIC statistics
plot(summary_reg_richness$cp, xlab = " Number of Variables ",
ylab = "Cp", type = "l")
# plot Cp and BIC statistics
plot(summary_reg_richness$cp, xlab = " Number of Variables ",
ylab = "Cp", type = "l")
# indicate model with smallest statistic
which.min(summary_reg_richness$cp)
# 3
points(3, summary_reg_richness$cp[3], col = " red ", cex = 2,
pch = 20)
# 3
plot(summary_reg_richness$bic , xlab = " Number of Variables ",
ylab = " BIC ", type = "l")
points (3, summary_reg_richness$bic[3], col = " red ", cex = 2,
pch = 20)
plot(reg_richness, scale = "r2")
plot(reg_richness, scale = "adjr2")
plot(reg_richness, scale = "Cp")
plot(reg_richness, scale = "bic")
# model with lowest BIC contains exposure, salinity, and NAP
## use coef() to see coefficient estimates associated with this model
coef(reg_richness, 3)
# perform 'forward' and 'backward' stepwise selections
fwd_richness <- regsubsets (richness ~ ., survey_data_richness, nvmax = 15, method = "forward")
summary(fwd_richness)
# exposure and NAP appear to be most significant variables
bwd_richness <- regsubsets (richness ~ ., survey_data_richness, nvmax = 15, method = "backward")
summary(bwd_richness)
# test for best 7-variable models
coef(reg_richness, 7)
coef(fwd_richness, 7)
coef(bwd_richness, 7)
summary(bwd_richness)
# test for best 7-variable models
coef(reg_richness, 7)
coef(fwd_richness, 7)
coef(bwd_richness, 7)
# validation set approach
## split data into training set and test set
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(survey_data_richness), replace = TRUE)
test = (!train)
# apply regsubsets() to training set to perform best subset selection
reg_best = regsubsets(richness ~., survey_data_richness[train, ], nvmax=15)
# make a model matrix from test data
test_mat = model.matrix(richness ~., survey_data_richness[train, ])
# for each size 'i', extract coefficients from reg_best for best model of that size
## multiply coefficients into appropriate columns of test model matrix to form predictions
val_errors = rep(NA, 14)
for (i in 1:14) {
coefi = coef(reg_best, id = i)
pred = test_mat[, names(coefi)] %*% coefi
val_errors[i] = mean((survey_data_richness$richness[test] - pred)^2)
}
val_errors
which.min(val_errors)
# 5 variables minimize the error
coef(reg_best, 5)
# create a 'predict' method using the previous function
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
xvars = names(coefi)
mat[, xvars] %*% coefi
}
# create a 'predict' method using the previous function
predict.regsubsets = function(object, newdata, id, ...) {
form = as.formula(object$call[[2]])
mat = model.matrix(form, newdata)
coefi = coef(object, id = id)
xvars = names(coefi)
mat[, xvars] %*% coefi
}
# run best subset selection on full dataset to select best 8-variable model
regfit.best = regsubsets(richness ~., survey_data_richness, nvmax=15)
coef(regfit.best, 8)
# choose among models of different sizes using cross-validation
## perform best subset selection within each of the k training sets
### create k folds
k=10
n=nrow(survey_data_richness)
set.seed(1)
# create matrix to store results
folds = sample(rep(1:k, length = n))
cv.errors = matrix(NA, k , 15, dimnames = list(NULL, paste(1:15)))
# write a function that performs cross-validation
## elements of folds that equal 'j' are in the test set
for (j in 1:k) {
best.fit = regsubsets(richness ~., survey_data_richness[folds != j, ], nvmax=15)
for (i in 1:14) {
pred = predict(best.fit, survey_data_richness[folds ==j, ], id = i)
cv.errors[j, i] = mean((survey_data_richness$richness[folds == j] - pred)^2)
}
}
# average over matrix columns to find a vector for which ith element is the cross-validation error for the i-variable model
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")
plot(mean.cv.errors, type = "b", ylim = [-100,100])
plot(mean.cv.errors, type = "b", ylim = (-100,100)
plot(mean.cv.errors, type = "b", ylim = (-100,100))
plot(mean.cv.errors, type = "b")
# cross-validation selects a 2-variable model
## perform cross-validation on the full dataset to identify 2-variable model
reg.best = regsubsets(richness~., survey_data_richness, nvmax=15)
coef(reg.best, 2)
# best 2-variable model contains exposure and NAP as variables
names(reg.summary)
par (mfrow = c(2, 2))
plot (reg.summary$rss , xlab = " Number of Variables ",
ylab = " RSS ", type = "l")
plot (reg.summary$adjr2 , xlab = " Number of Variables ",
ylab = " Adjusted RSq ", type = "l")
which.max (reg.summary$adjr2)
points(4, reg.summary$adjr2[4], col = " red ", cex = 2,
pch = 20)
